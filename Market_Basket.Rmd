---
title: "Market Basket Analysis"
author: "Kostas Voulgaropoulos"
date: "3/5/2021"
output: pdf_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(lubridate)
library(arules)
library(arulesViz)
#library(plyr)
library(ggthemes)
library(plotly)
library(usethis)

use_git_config(user.name = "Kostas Voul", user.email = "voulkon93@gmail.com")

is_interactive <- FALSE

```

# Overview

The following report showcases a market basket analysis and its use to a retail store. Market basket analysis helps identify subtle connections among products and can be found extremely useful in product placement and marketing discount offerings.

The data used for this certain example is public, donated in 2015 to the UC Irvine Machine Learning Repository and can be found [here](https://archive.ics.uci.edu/ml/datasets/online+retail).

From the repository's description: *This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.*

After a basic preprocessing our dataset looks like this:

```{r cars, echo=FALSE}
file <- "C:\\Users\\kvoul\\Desktop\\Projects\\Retail_\\Online Retail.xlsx"
retail <- read_excel(file)

retail <- retail[complete.cases(retail), ]

retail <- retail %>% 
  mutate(#Description = as.factor(Description),
         Country = as.factor(Country),
         Date = as.Date(InvoiceDate),
         Time = format(InvoiceDate, "%H:%M:%S"))

head(retail)

```

# Exploratory Data Analysis

First, we need to explore a little bit our data so that we get a feeling of what to expect. In the EDA section, we seek to answer generic questions about our dataset.

## How many different items do customers buy during each visit?

In case most of the times our customers buy only one thing during their visit, a market basket analysis is not only difficult to perform (since associations exist in **groups** of products) but also **without much value**.


```{r echo = FALSE}

temp <- retail %>% 
  group_by(InvoiceNo) %>% 
  summarise( Products = n_distinct(StockCode) )

#
#Store Invoices with only 1 product in a variable to exclude later
#Single_product_invoices <- temp %>% filter(Products == 1) %>% pull(InvoiceNo)

plot1 <- temp %>%
  ggplot( aes(x = Products) ) + 
  geom_area(stat = "bin", binwidth = 1, fill = "#a6b6c9") +
  labs( title = "Products per Invoice", y = "Products per invoice" ) +
  theme_minimal()
  
if(is_interactive){plot1 %>% ggplotly()}else{plot1} 

table_of_products <- (temp$Products %>% table())

```

What we see from this graph is that:

-The **most frequent** number of items per invoice is **1** ( `r table_of_products[1]` Invoices) or at least close to 1

```{r echo = FALSE}


table_of_products %>% head(10)


```

-There are **extreme orders**, even an invoice with 541 (!) items

```{r echo = FALSE}


table_of_products %>% tail(10)

```

Which makes sense, since the shop also **deals with wholesale**. 
Wholesale buyers, especially those buying such a variety of products, may not be so representative of the retail scheme. They seek to **maintain a stock**, thus their invoices **do not reflect their own preferences**. 
But on the other hand, their purchases **reflect** their **customers' desires**, especially if we focus on the volume of the products they buy. They must be conservative on the lower-selling products and bold on the higher-selling.

Overall, there are plenty of invoices that contain more than one item, thus it is **possible** to conduct a **market basket analysis**.

But the fact that 1 is the most frequent number, highlights the need for some extra analysis on those orders.

## Best and Worst selling products (by both quantity and value) and how often are they purchased?

Before we seek to answer these questions, we need to **exclude invoices that represent returns** of products. 
If we omit this step, the worst selling products might just become the most frequently returned. 

Such data would be valuable for analyzing returns (e.g. detecting defective products), but the scope of this current analysis is limited to sales.

### Best Selling

```{r echo = FALSE}

temp <- retail %>%
  mutate(Value = UnitPrice*Quantity) %>% 
  filter( UnitPrice > 0) %>% filter(Quantity > 0) %>% filter(Value > 0 ) %>% 
  group_by(Description) %>%
  summarise( Quantity = sum(Quantity),
             Value = sum(Value))


value_green <- "#2a6e35"
quant_blue <- "#3a5491"


## Top 5 - Worst 5 to get a feeling of the numbers ###

plot2a <- 
  temp %>% 
    arrange(desc(Quantity)) %>% 
    head(10) %>% #top_n(x = 10, wt = Quantity) %>%
    ggplot(aes( x = reorder(Description, Quantity), y = Quantity )) + 
    geom_bar(stat = "identity",width = 0.4, fill = quant_blue  ) + 
    labs(title = "Best Selling Products by Quantity", x = "", y = "Quantity") +
    coord_flip() +
    theme_minimal()


if(is_interactive){style(plot2a, hoverinfo = "x")}else{plot2a}


plot2b <- 
  temp %>% 
    arrange(desc(Value)) %>% 
    head(10) %>% #top_n(x = 10, wt = Quantity) %>%
    ggplot(aes( x = reorder(Description, Value), y = Value )) + 
    geom_bar(stat = "identity",width = 0.4, fill = value_green  ) + 
    labs(title = "Best Selling Products by Value", x = "", y = "Value") +
    coord_flip() +
    theme_minimal()

if(is_interactive){style(plot2b, hoverinfo = "x")}else{plot2b}


```


### Worst Selling

```{r echo = FALSE}

plot2c <- 
  temp %>% 
    arrange((Quantity)) %>% 
    head(10) %>% #top_n(x = 10, wt = Quantity) %>%
    ggplot(aes( x = reorder(Description, -Quantity), y = Quantity )) + 
    geom_bar(stat = "identity",width = 0.4, fill = quant_blue ) + 
    labs(title = "Worst Selling Products by Quantity", x = "", y = "Quantity") +
    coord_flip() +
    theme_minimal()

if(is_interactive){style(plot2c, hoverinfo = "x")}else{plot2c}

```

All Bottom 10 Products have only **sold one piece** over a period of years .

How many products have this fate?

```{r echo = FALSE}


temp %>% filter(Quantity == 1) %>% nrow()


```

59 distinct products have moved only once! 
Pretty useful information, especially if we seek to optimize workload exercise.

So, excluding all those laggers, the above graph might look like:

```{r echo = FALSE}
plot2c <- 
  temp %>%  
    arrange((Quantity)) %>% 
    filter(Quantity != 1) %>%
    head(10) %>% #top_n(x = 10, wt = Quantity) %>%
    ggplot(aes( x = reorder(Description, -Quantity), y = Quantity )) + 
    geom_bar(stat = "identity",width = 0.4, fill = quant_blue ) + 
    labs(title = "Worst Selling Products by Quantity", x = "", y = "Quantity") +
    coord_flip() +
    theme_minimal()

if(is_interactive){style(plot2c, hoverinfo = "x")}else{plot2c}

```

Now we're stuck in the neighborhood of **2 moves** over the course of those years.

Perhaps a density plot could show us what to expect in the aforementioned neighborhood of low selling products:

```{r echo = FALSE}


plot2e <- temp %>%
  
  filter(Quantity <600) %>%

  ggplot(aes(x = Quantity)) +
  
  geom_histogram( binwidth = 1, fill =  "#3a8491")+
  
  labs(title = "Histogram of Product Codes per Pieces Sold", 
       subtitle = "Products sold more than 600 pieces excluded", 
       x = "Products Sold",
       y = "Number of Product Codes") + 
  theme_minimal() 
  
if(is_interactive){plotly::ggplotly(plot2e)}else{plot2e}


```

And bingo! There are way too **many slow moving** products.

`r temp %>% filter( Quantity == 1 ) %>% nrow()` distinct codes have sold only 1 piece, `r temp %>% filter( Quantity == 2 ) %>% nrow()` codes sold 2 pieces, and the list goes on and on.

Unfortunately, with such measures, looking at the **10 lowest selling products** is of **no value**, since the low selling are way more than 10. 

Let's now look at the worst value bringers:


```{r echo = FALSE}


plot2d <- 
  temp %>% 
    arrange((Value)) %>% 
    head(10) %>% #top_n(x = 10, wt = Quantity) %>%
    ggplot(aes( x = reorder(Description, -Value), y = Value )) + 
    geom_bar(stat = "identity",width = 0.4, fill = "#ad343c" ) + 
    labs(title = "Worst Selling Products by Value", x = "", y = "Value") +
    coord_flip() +
    theme_minimal()

if(is_interactive){style(plot2d, hoverinfo = "x")}else{plot2d}


```

There are products that have generated **less than 1 pound of revenue over years**. Perhaps a retailer might gain insights from that.

Though it might not be of obvious use, knowing the best and worst sellers can be effectively **combined** with a market basket analysis and **affect the marketing strategy** of each retailer. 

For example, a well performing product might prove associated with both one well and one poorly performing one. 
The retailer might decide to connect it with the poorly performing in order to **boost its sales** or totally **abandon** the hopeless poor performer and target the **boosting of the two well performing** products by combining them.

One way or another, the overall performance of a product is useful as a context in making decisions from a market basket analysis.

### Products of Highest and Lowest Effort

Expanding the value of context, a retailer will surely find useful knowing which products generate revenue without needing much warehouse circulation (and thus effort) and which don't.

That's why a look at the average revenue per product would be insightful.

```{r echo = FALSE}


temp <- temp %>% 
  filter(Quantity != 0) %>%
  mutate( Average_Value = Value / Quantity )


plot3a <- 
  temp %>% 
    arrange(desc(Average_Value)) %>% 
    head(10) %>% #top_n(x = 10, wt = Quantity) %>%
    ggplot(aes( x = reorder(Description, Average_Value), y = Average_Value )) + 
  
    geom_point(color="#1e5c2b", fill= "#428052" , alpha=1, shape=21, stroke=2 ) + 
    
    geom_segment(aes(x=reorder(Description, Average_Value), 
                     xend=reorder(Description, Average_Value), y=0, yend=Average_Value) , size = 1.2, ) + 
  
    labs(title = "Highest Values per Product", x = "", y = "Value_per_Product") +
  
    coord_flip() +
    
    theme_minimal()

if(is_interactive){style(plot3a, hoverinfo = "x")}else{plot3a}

plot3b <- 
  temp %>% 
  filter(Average_Value>0) %>%
    arrange((Average_Value)) %>% 
  
    head(10) %>% #top_n(x = 10, wt = Quantity) %>%
    ggplot(aes( x = reorder(Description, -Average_Value), y = Average_Value )) + 
  
    geom_point(color="#a12d10", alpha=2, shape=21, stroke=2 ) + 
    
    geom_segment(aes(x=reorder(Description, Average_Value), 
                     xend=reorder(Description, Average_Value), y=0, yend=Average_Value) , size = 1.2, ) + 
  
    labs(title = "Lowest Values per Product", x = "", y = "Value_per_Product") +
  
    coord_flip() +
    
    theme_minimal()

if(is_interactive){style(plot3b, hoverinfo = "x")}else{plot3b}



```

The above insight can be evaluated in context with the results of the market basket analysis.

A store owner could for example choose to prioritize rules that include low effort products over others with higher effort products. 

But enough with the Exploratory Analysis and the Graphs, let's go on with mining association rules.

# Market Basket Analysis

Market Basket Analysis is based on [**Association Rule Learning**](https://en.wikipedia.org/wiki/Association_rule_learning), a machine learning technique that helps us identify rules in large datasets.

An example of an interesting rule (expressed in the form of if product X is then Y is also bought) is the ["Beer and Diapers Correlation"](https://tdwi.org/articles/2016/11/15/beer-and-diapers-impossible-correlation.aspx) identified in the early 90's.

Even though it hasn't reached consensus, the most prevalent explanation of this rule is that fathers that to go buy diapers for their children in the evening after work, reward themselves with a six-pack of beers.

In the present analysis, we will use the [**Apriori Algorithm**](https://en.wikipedia.org/wiki/Apriori_algorithm) to mine associations hidden in our dataset.

Apriori calculates **certain parameters**, which are better understood with the following example:

Let's say a sports store has:
100 transactions (or invoices) in total, 
13 of which contain a tennis **racket**, 
12 contain tennis **balls** and
10 contain **both** a racket and balls

Between the tennis racket (product A) and the balls (product B), there is:  

1. **Support** of 0.1 or 10% (= 10 / 100), which represents the **Frequency of the combination** of interest (10) divided by the **total transactions** (100). 

2. **Confidence** of 0.77 or 77% (= 10 / 13), which represents the **Frequency of the combination** of interest (10) divided by the **Frequency of Product A** (13)

3. **Lift** of 6.41 (= 0.77 / (12/100) ),  which represents the increase in the ratio of sale of B (12/100 in our example) when A is sold. In other words, lift shows that the **likelihood of buying** a racket **and** balls **together** is 6.41 times more than buying the **balls alone**.
Lifts greater than 0 signify that the two products are **more likely to be bought together** than separately, and vice versa (Lift < 0 shows that separate sales are more likely than those of the group).

4. **Coverage** of 0.13 (= 0.1 / 0.77 or 13/100 ). Coverage is also called left-hand-side support and represents a measure of **how often** the rule can be applied. In practice, it shows us **how often product B appears**.

So, before we train our APRIORI algorithm, we need to specify which kind of rules we would like to see.



```{r echo =FALSE}


transactions <- retail %>% 
  filter(Quantity > 0) %>% 
  mutate(Dummy = TRUE,
         InvoiceNo = paste0(InvoiceNo, InvoiceDate)) %>% 
  pivot_wider( names_from = "Description", 
               values_from = "Dummy" ,
               id_cols = "InvoiceNo",
               values_fn = length)


transactions <- transactions[,-1]

transactions[is.na(transactions)] <- FALSE

transactions[(transactions) > 1] <- 1


tr <- as(as.matrix(transactions),"transactions")

#summary(transactions2)

support <- 0.0095
confidence <- 0.9
maxlength <- 8


```

In this certain analysis we will be strict with our rules and choose to only see rules with:

1. **Support** greater than `r support` or `r round(support*100,2)`

2. **Confidence** greater than `r confidence` or `r round(confidence*100,2)`

3. **Maximum Length** less or equal to `r maxlength`. That means we don't care for combinations that contain more than `r maxlength` products.

Now that we have tuned our algorithm's parameters, let's go on and observe results:

```{r results, echo=FALSE}

association.rules <- apriori(tr, parameter = list(supp = support, 
                                                  conf = confidence, 
                                                  maxlen = maxlength))

#summary(association.rules)
inspect(association.rules)

```
And Voila!

These **`r length(association.rules)` combinations** satisfy the criteria set earlier.

```{r an example, echo = FALSE}

itms <- labels(association.rules[1],itemSep = " + ", setStart = "", setEnd="", 
  ruleSep = "-") %>% str_split("-") %>% unlist()



```


We can see for example that: 

1. `r itms[1]` is combined with `r itms[2]` in `r round(inspect(association.rules[1])[[4]] *100,2)`% of the transactions.

2. `r round(inspect(association.rules[1])[[5]] *100, 2)`% of the times `r itms[2]` was bought, `r itms[1]` was also bought

3. `r itms[2]` is present in `r round(inspect(association.rules[1])[[6]] *100,2)`% of total invoices

4. it is `r round(inspect(association.rules[1])[[7]],2)` times more probable for `r itms[2]` to be **bought along with** `r itms[1]`, than without it.

5. There have been `r inspect(association.rules[1])[[8]]` invoices that contain both `r itms[1]` and `r itms[2]`

# Conclusion

We've come to analyze our dataset and, apart from the insights gained from the Exploratory Data Analysis with the graphs, we managed to extract rules useful for marketing strategy and product placement. 

Such analyses are **sensitive** to each user's **preferences** and **goals**. Our tuning parameters (minimum support and confidence) were more or less picked by chance. 

On top of it, the few (with such strict criteria) rules that we unearthed, we did not convert them to actionable advice. This is due to the fact that such information needs to be filtered under certain **business criteria** and **priorities** that vary from retailer to retailer.

For example, one retailer may seek to **minimize their warehouse cost**, another one to **maximize revenue**, another to **optimize orders' volume and frequency**,  another to **attract new customers**, etc. What is useful to the one, might be meaningless for the other. 

Rules exist and each recipient **utilizes** them in a **manner** that **suits them best**.

